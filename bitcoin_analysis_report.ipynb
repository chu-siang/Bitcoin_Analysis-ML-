{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7f52fb",
   "metadata": {},
   "source": [
    "# Bitcoin Price Analysis and Prediction Using Machine Learning\n",
    "\n",
    "## Web link to dataset\n",
    "[GitHub Repository](https://github.com/chu-siang/Bitcoin_Analysis_ML/tree/main)\n",
    "\n",
    "[GitHub Repository (raw data)](https://github.com/chu-siang/Bitcoin_Analysis_ML/blob/main/data/raw/bitcoin_raw_data.csv)\n",
    "\n",
    "[GitHub Repository (preprocessed data)](https://github.com/chu-siang/Bitcoin_Analysis_ML/blob/main/data/processed/bitcoin_preprocessed.csv)\n",
    "\n",
    "[GitHub Repository (clusters data)](https://github.com/chu-siang/Bitcoin_Analysis_ML/blob/main/data/processed/bitcoin_clusters.csv)\n",
    "\n",
    "[GitHub Repository (features data)](https://github.com/chu-siang/Bitcoin_Analysis_ML/blob/main/data/processed/bitcoin_features.csv)\n",
    "\n",
    "## Research Question\n",
    "This research investigates how effectively machine learning models can predict Bitcoin price movements based on historical price data and technical indicators using 1-hour intervals over a 4-month period (November 2024 to March 2025). Additionally, we explore whether unsupervised learning can identify distinct market states with different behavior patterns.\n",
    "\n",
    "## Dataset Documentation\n",
    "\n",
    "### Data Type and External Source\n",
    "The dataset consists of Bitcoin (BTC/USDT) price data collected from the Binance API, including 1-hour candlestick information from November 1, 2024, to March 1, 2025. The raw data includes time-series records of Bitcoin's price and trading volume, with each record containing a timestamp and OHLCV (Open, High, Low, Close, Volume) values.\n",
    "\n",
    "### Dataset Preprocessing and Feature Engineering\n",
    "I conducted several preprocessing steps to ensure data quality:\n",
    "- Converting timestamps to datetime format\n",
    "- Ensuring consistent hourly intervals\n",
    "- Removing missing timestamps and sorting chronologically\n",
    "- Creating technical indicators as features\n",
    "- Generating target variables for prediction\n",
    "\n",
    "From the raw data, I derived over 30 technical indicators and features including:\n",
    "- Simple Moving Averages (SMA): 12-hour SMA for smoothing short-term fluctuations\n",
    "- Exponential Moving Averages (EMA): 6h, 12h, 24h EMAs emphasizing recent price trends\n",
    "- Relative Strength Index (RSI): 14-period RSI signaling overbought (>70) or oversold (<30) conditions\n",
    "- Moving Average Convergence Divergence (MACD): Trend momentum indicators\n",
    "- Bollinger Bands: For volatility and extreme price deviation detection\n",
    "- Volatility measures: 24-hour volatility and volume-related features\n",
    "- Time-based features: Hour of day (0-23) and weekend flags (1 for Saturday/Sunday, 0 otherwise)\n",
    "\n",
    "### Target Variable Construction\n",
    "For supervised learning, we created the following targets:\n",
    "- 24-hour future return (return_24h): Percentage change from current time (t) to 24 hours later (t+24h)\n",
    "- Binary price direction (price_up_24h): 1 if future return positive, 0 otherwise\n",
    "- Future volatility (future_volatility_24h): For cluster analysis only, not prediction\n",
    "\n",
    "![Bitcoin Price with Cluster Classifications](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/cluster_time_series.png)\n",
    "*Figure 1: Bitcoin price chart with identified market state clusters. Each point represents Bitcoin price colored by its assigned cluster, showing how market states evolve over time from November 2024 to March 2025. Purple points (Cluster 0) appear during sideways movement, green points (Cluster 2) during uptrends, blue points (Cluster 1) during corrections, and yellow points (Cluster 3) often after price drops.*\n",
    "\n",
    "## Description of Supervised and Unsupervised Methods\n",
    "\n",
    "### Supervised Learning Methods\n",
    "\n",
    "#### 1. Random Forest Regression\n",
    "Random Forest is an ensemble learning method that constructs multiple decision trees and outputs the average prediction to improve accuracy and control overfitting.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Framework: scikit-learn's RandomForestRegressor\n",
    "- Features: 30+ technical indicators\n",
    "- Target: 24-hour future returns\n",
    "- Hyperparameters: 100 trees, default depth\n",
    "\n",
    "![Random Forest Feature Importance](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/rf_feature_importance.png)\n",
    "*Figure 2: Top 15 feature importances in the Random Forest model. The chart shows which features most influenced predictions, with price_up_24h and future_volatility_24h having the highest importance scores (indicating some target leakage). Among legitimate technical indicators, Bollinger Bands (bb_upper) and moving averages (sma_24h) contributed the most to predictions.*\n",
    "\n",
    "#### 2. Support Vector Regression (SVR)\n",
    "SVR finds a function that best predicts the continuous output value for a given input value, while maximizing the margin.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Framework: scikit-learn's SVR\n",
    "- Features: Same technical indicators used for Random Forest\n",
    "- Target: 24-hour future returns\n",
    "- Hyperparameters: RBF kernel, C=10, epsilon=0.1\n",
    "\n",
    "### Unsupervised Learning Method\n",
    "\n",
    "#### K-means Clustering\n",
    "K-means clustering was applied to identify distinct market states or regimes in Bitcoin trading patterns by grouping data into clusters that minimize intra-cluster variance.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Framework: scikit-learn's KMeans\n",
    "- Features: Selected subset including returns, volatility, RSI, volume ratio, MACD, Bollinger Band width\n",
    "- Parameters: 4 clusters (k=4)\n",
    "- No future/target information was included in clustering\n",
    "\n",
    "![K-means Clustering Visualization](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/kmeans_clusters.png)\n",
    "*Figure 3: K-means clustering of Bitcoin market states visualized in principal component space. Each point represents an hourly observation projected onto two principal components and colored by cluster. The visualization shows clear separation between the four market states: purple points (Cluster 0) forming a dense group at bottom-left, green points (Cluster 2) spreading to the right, blue points (Cluster 1) in the upper-middle region, and yellow points (Cluster 3) appearing in the upper-left.*\n",
    "\n",
    "## Description of Experiments and Evaluation Results\n",
    "\n",
    "### Experiment 1: Regression Performance and Cross-Validation\n",
    "I compared the performance of Random Forest and SVR models in predicting 24-hour Bitcoin price returns, using chronological train-test splitting and 5-fold cross-validation.\n",
    "\n",
    "**Results:**\n",
    "The Random Forest significantly outperformed SVR, achieving lower MSE and better R² values. However, both models had negative R² scores on the test set, indicating limited predictive power for the highly volatile Bitcoin returns. The RF partially captured directional movements, while SVR predicted mostly near-zero returns.\n",
    "\n",
    "![Model Prediction Comparison](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/actual_vs_predicted.png)\n",
    "*Figure 4: Scatter plots of actual vs. predicted 24h returns for Random Forest (left) and SVR (right). The red dashed line represents perfect prediction. The Random Forest shows predictions somewhat correlated with actual returns, while SVR predictions cluster horizontally near zero, indicating its failure to capture return variability.*\n",
    "\n",
    "![Error Distribution](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/error_distribution.png)\n",
    "*Figure 5: Histograms showing prediction error distributions for Random Forest (left) and SVR (right). The RF errors are more tightly centered around zero with fewer extreme errors, while SVR shows a broader, skewed distribution, confirming its tendency to underpredict returns.*\n",
    "\n",
    "![Bitcoin Price Return Predictions](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/model_predictions.png)\n",
    "*Figure 6: Time series of actual Bitcoin returns (blue) compared with Random Forest predictions (orange) and SVR predictions (green) during February 2025. The RF partially tracks actual return patterns but underestimates extreme movements, while SVR predictions remain near zero throughout, demonstrating its limited predictive power.*\n",
    "\n",
    "### Experiment 2: Data Augmentation Effects\n",
    "I investigated four augmentation techniques beyond the baseline (no augmentation):\n",
    "- Gaussian noise addition (std=0.01, 0.05, 0.1)\n",
    "- Synthetic sample mixing (averaging random pairs of training examples)\n",
    "\n",
    "**Results:**\n",
    "Moderate Gaussian noise (std=0.05) yielded the best improvement, reducing MSE from 0.00075 to 0.00071 and improving R² from -0.8 to -0.7. This suggests that adding controlled noise can act as a regularizer, helping the model generalize better. Smaller noise levels were ineffective, while synthetic mixing did not help.\n",
    "\n",
    "![Data Augmentation Results](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/experiments/data_augmentation_experiment.png)\n",
    "*Figure 7: Bar charts comparing MSE (left) and R² (right) for different data augmentation methods. The baseline shows moderate performance, tiny noise (std=0.01) slightly worsens results, moderate noise (std=0.05) gives the best performance with lowest MSE and highest R², while higher noise (std=0.1) and synthetic mixing show results similar to baseline.*\n",
    "\n",
    "### Experiment 3: PCA Dimensionality Reduction\n",
    "I tested how using principal component analysis (PCA) to reduce feature dimensions affected model performance.\n",
    "\n",
    "**Results:**\n",
    "PCA dimensionality reduction degraded model performance. Using fewer components (e.g., 3) significantly increased MSE and worsened R². Performance gradually improved as more components were added, approaching but not exceeding the original feature set performance. This indicates that the full feature set contains valuable information not captured by the first few principal components.\n",
    "\n",
    "![PCA Experiment Results](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/experiments/pca_experiment.png)\n",
    "*Figure 8: Effects of PCA dimensionality reduction on model performance. The top-left chart shows MSE increasing dramatically with fewer components and gradually improving as components increase. Top-right shows R² following the opposite pattern. Bottom-left relates MSE to variance explained, and bottom-right displays the component loadings heatmap showing how original features contribute to principal components.*\n",
    "\n",
    "### Experiment 4: Market State Clustering\n",
    "I analyzed the four market states identified by K-means clustering to understand their characteristics and temporal distribution.\n",
    "\n",
    "**Results:**\n",
    "The clustering revealed four distinct market regimes with clear differences in future returns, volatility, RSI, and volume:\n",
    "\n",
    "- **Cluster 0 (Purple)**: \"Calm Market\" - Low returns (~0.05%), neutral RSI (~50), lowest volatility (~0.022) and volume (~800). Represents sideways, low-activity periods.\n",
    "\n",
    "- **Cluster 1 (Blue)**: \"Correction Phase\" - Moderate returns (~0.25%), relatively low RSI (~35-40), high volatility (~0.028), moderate volume (~1500). Represents recovering or dipping markets.\n",
    "\n",
    "- **Cluster 2 (Green)**: \"Bull Market\" - High returns (~0.30%), very high RSI (~70), highest volatility (~0.030), high volume (~2500). Represents bullish trending states.\n",
    "\n",
    "- **Cluster 3 (Yellow)**: \"Reversal State\" - Highest returns (~0.35%), lowest RSI (~30), high volatility (~0.028), highest volume (~3200). Represents post-crash rebound situations.\n",
    "\n",
    "![PCA Visualization of Clusters](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/cluster_pca.png)\n",
    "*Figure 9: Alternative PCA visualization of the Bitcoin market state clusters. This projection shows the distinctive distribution of cluster points across the principal component space with different variance scaling, highlighting outlier points and revealing the large-scale structure of the dataset.*\n",
    "\n",
    "![Cluster Characteristics](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/cluster_statistics.png)\n",
    "*Figure 10: Bar charts showing key statistical properties of each cluster. Top-left shows average 24h future returns (highest in Cluster 3); top-right shows return volatility (lowest in Cluster 0); bottom-left shows average RSI (highest in Cluster 2); bottom-right shows trading volume (highest in Cluster 3). These metrics reveal the distinct nature of each market state.*\n",
    "\n",
    "The temporal distribution of clusters aligned with observable market behavior. Green cluster points appeared during strong uptrends, yellow cluster points often followed local price minima, blue cluster points appeared during corrections, and purple cluster points predominated during flat periods.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Predictive Challenge**: Both supervised models struggled to predict exact 24-hour returns (negative R²), with Random Forest performing better than SVR. This aligns with the efficient market hypothesis that short-term price movements are difficult to predict.\n",
    "\n",
    "2. **Technical Indicators**: No single technical indicator strongly predicted future returns. The model relied on combinations of features with Bollinger Bands and moving averages showing modest predictive value.\n",
    "\n",
    "3. **Data Augmentation**: Adding moderate Gaussian noise (5-10% of feature scale) during training improved model performance slightly, suggesting it helps mitigate overfitting.\n",
    "\n",
    "4. **Feature Dimensionality**: PCA dimensionality reduction decreased performance, indicating the model benefits from the full feature space and complex feature interactions.\n",
    "\n",
    "5. **Market State Identification**: Unsupervised clustering successfully identified four meaningful market regimes: calm sideways markets, corrections, bullish trends, and post-crash reversals. These clusters showed distinct characteristics in returns, volatility, RSI, and volume.\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "1. **Advanced Models**: Test deep learning approaches like LSTM networks that might better capture temporal dependencies in price data.\n",
    "\n",
    "2. **Additional Data**: Incorporate sentiment analysis, on-chain metrics, or macroeconomic indicators to provide broader context.\n",
    "\n",
    "3. **Alternative Approaches**: Reframe the prediction task as classification rather than regression to potentially achieve better results.\n",
    "\n",
    "4. **Time Sequence Modeling**: Explore sequence models that account for autocorrelation in returns and indicators over time.\n",
    "\n",
    "5. **Practical Applications**: While direct return prediction remains challenging, the market state clustering offers practical value for risk management and trading strategy adaptation.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Random forests. Machine Learning :  https://medium.com/chung-yi/ml%E5%85%A5%E9%96%80-%E5%8D%81%E4%B8%83-%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-6afc24871857\n",
    "2. Support-vector networks. Machine Learning. https://scikit-learn.org/stable/modules/svm.html \n",
    "3. Scikit-learn: Machine Learning in Python. https://scikit-learn.org/stable/\n",
    "4. pandas: a Foundational Python Library for Data Analysis. https://pandas.pydata.org/ \n",
    "5. Binance API Documentation. https://binance-docs.github.io/apidocs/\n",
    "\n",
    "## Appendix: Project Structure\n",
    "\n",
    "The repository contains the following key files and directories:\n",
    "\n",
    "```\n",
    "bitcoin-analysis/\n",
    "├── README.md\n",
    "├── requirements.txt\n",
    "├── Makefile\n",
    "├── data/\n",
    "│   ├── raw/\n",
    "│   │   └── bitcoin_raw_data.csv\n",
    "│   └── processed/\n",
    "│       ├── bitcoin_features.csv\n",
    "│       └── bitcoin_ml_data.csv\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── data/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── fetch_data.py\n",
    "│   │   └── preprocess.py\n",
    "│   ├── features/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── create_features.py\n",
    "│   │   └── create_targets.py\n",
    "│   ├── models/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── random_forest.py\n",
    "│   │   ├── svr.py\n",
    "│   │   └── kmeans.py\n",
    "│   └── visualization/\n",
    "│       ├── __init__.py\n",
    "│       ├── plot_predictions.py\n",
    "│       ├── plot_clusters.py\n",
    "│       └── plot_experiments.py\n",
    "├── experiments/\n",
    "│   ├── __init__.py\n",
    "│   ├── data_size.py\n",
    "│   ├── data_augmentation.py\n",
    "│   └── dimensionality_reduction.py\n",
    "├── models/\n",
    "│   ├── random_forest_model.pkl\n",
    "│   ├── svr_model.pkl\n",
    "│   ├── kmeans_model.pkl\n",
    "│   ├── optimized_random_forest_model.pkl\n",
    "│   └── optimized_svr_model.pkl\n",
    "├── results/\n",
    "│   ├── figures/\n",
    "│   │   ├── prediction_comparison.png\n",
    "│   │   ├── feature_importance.png\n",
    "│   │   ├── kmeans_clusters.png\n",
    "│   │   ├── training_size_experiment.png\n",
    "│   │   ├── augmentation_experiment.png\n",
    "│   │   ├── pca_experiment.png\n",
    "│   │   └── cluster_analysis.png\n",
    "│   └── metrics/\n",
    "│       └── model_performance.csv\n",
    "└── report/\n",
    "    ├── bitcoin_analysis_report.pdf\n",
    "    └── figures/\n",
    "        ├── price_chart.png\n",
    "        ├── model_comparison.png\n",
    "        └── cluster_visualization.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34036655",
   "metadata": {},
   "source": [
    "## Appendix: Code PART !!!\n",
    "\n",
    "### 1. Fetch Data\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# Make sure data directories exist\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "def fetch_bitcoin_data(start_date, end_date, interval='1h'):\n",
    "    \"\"\"\n",
    "    Fetch Bitcoin price data from a public API.\n",
    "    Params:\n",
    "        start_date: Start date in YYYY-MM-DD format\n",
    "        end_date: End date in YYYY-MM-DD format\n",
    "        interval: Data granularity (1h for 1-hour data)\n",
    "    Returns:\n",
    "        DataFrame with OHLCV data\n",
    "    \"\"\"\n",
    "    # Convert dates to timestamps\n",
    "    start_ts = int(datetime.datetime.strptime(start_date, '%Y-%m-%d').timestamp() * 1000)\n",
    "    end_ts = int(datetime.datetime.strptime(end_date, '%Y-%m-%d').timestamp() * 1000)\n",
    "    \n",
    "    # Binance API endpoint for historical klines (candlestick) data\n",
    "    url = 'https://api.binance.com/api/v3/klines'\n",
    "    \n",
    "    # Parameters for API request\n",
    "    params = {\n",
    "        'symbol': 'BTCUSDT',\n",
    "        'interval': interval,\n",
    "        'startTime': start_ts,\n",
    "        'endTime': end_ts,\n",
    "        'limit': 1000  # Max limit per request\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Fetch data in chunks if needed\n",
    "    while start_ts < end_ts:\n",
    "        params['startTime'] = start_ts\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            break\n",
    "            \n",
    "        all_data.extend(data)\n",
    "        \n",
    "        # Update start_ts for next iteration\n",
    "        start_ts = data[-1][0] + 1\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', \n",
    "                                         'close_time', 'quote_asset_volume', 'trades', \n",
    "                                         'taker_buy_base', 'taker_buy_quote', 'ignored'])\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    \n",
    "    # Convert price columns to float\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        df[col] = df[col].astype(float)\n",
    "    \n",
    "    # Set timestamp as index\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    return df[['open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Fetching Bitcoin price data...\")\n",
    "    # Fetch 4 months of 1-hour Bitcoin data\n",
    "    start_date = '2024-11-01'\n",
    "    end_date = '2025-03-01'\n",
    "    bitcoin_data = fetch_bitcoin_data(start_date, end_date)\n",
    "    \n",
    "    # Save the raw data\n",
    "    bitcoin_data.to_csv('data/raw/bitcoin_raw_data.csv')\n",
    "    print(f\"Raw data saved to data/raw/bitcoin_raw_data.csv ({len(bitcoin_data)} rows)\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79f735",
   "metadata": {},
   "source": [
    "### 2. Preprocess Data\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"\n",
    "    Load raw Bitcoin price data.\n",
    "    \"\"\"\n",
    "    file_path = 'data/raw/bitcoin_raw_data.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Raw data file not found at {file_path}. Run fetch_data.py first.\")\n",
    "    \n",
    "    return pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preprocessing raw data...\")\n",
    "    # Load raw data\n",
    "    bitcoin_data = load_raw_data()\n",
    "    \n",
    "    # Perform basic preprocessing (if needed)\n",
    "    # Remove duplicate rows\n",
    "    bitcoin_data = bitcoin_data.drop_duplicates()\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    bitcoin_data = bitcoin_data.sort_index()\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    bitcoin_data.to_csv('data/processed/bitcoin_preprocessed.csv')\n",
    "    print(f\"Preprocessed data saved to data/processed/bitcoin_preprocessed.csv ({len(bitcoin_data)} rows)\")\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bfc93",
   "metadata": {},
   "source": [
    "### 3. Create Features\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    \"\"\"\n",
    "    Load preprocessed Bitcoin price data.\n",
    "    \"\"\"\n",
    "    file_path = 'data/processed/bitcoin_preprocessed.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Preprocessed data file not found at {file_path}. Run preprocess.py first.\")\n",
    "    \n",
    "    return pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create technical indicators and features for Bitcoin price prediction.\n",
    "    \"\"\"\n",
    "    # Make a copy of the dataframe\n",
    "    data = df.copy()\n",
    "    print(f\"Initial data shape: {data.shape}\")\n",
    "    \n",
    "    # Price-based features\n",
    "    data['returns'] = data['close'].pct_change()\n",
    "    data['log_returns'] = np.log(data['close'] / data['close'].shift(1))\n",
    "    \n",
    "    # Volatility features\n",
    "    data['volatility_1h'] = data['returns'].rolling(window=1).std() * np.sqrt(24)\n",
    "    data['volatility_24h'] = data['returns'].rolling(window=24).std() * np.sqrt(24)\n",
    "    \n",
    "    # Simple Moving Averages\n",
    "    data['sma_6h'] = data['close'].rolling(window=6).mean()\n",
    "    data['sma_12h'] = data['close'].rolling(window=12).mean()\n",
    "    data['sma_24h'] = data['close'].rolling(window=24).mean()\n",
    "    \n",
    "    # Exponential Moving Averages\n",
    "    data['ema_6h'] = data['close'].ewm(span=6, adjust=False).mean()\n",
    "    data['ema_12h'] = data['close'].ewm(span=12, adjust=False).mean()\n",
    "    data['ema_24h'] = data['close'].ewm(span=24, adjust=False).mean()\n",
    "    \n",
    "    # MACD\n",
    "    data['macd'] = data['ema_12h'] - data['ema_24h']\n",
    "    data['macd_signal'] = data['macd'].ewm(span=9, adjust=False).mean()\n",
    "    data['macd_hist'] = data['macd'] - data['macd_signal']\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = data['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    \n",
    "    # Check if loss contains any zeros before division\n",
    "    if (loss == 0).any():\n",
    "        print(\"Warning: Division by zero in RSI calculation\")\n",
    "        # Replace zeros with a small value to avoid division by zero\n",
    "        loss = loss.replace(0, 1e-10)\n",
    "    \n",
    "    rs = gain / loss\n",
    "    data['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    data['bb_middle'] = data['close'].rolling(window=20).mean()\n",
    "    data['bb_std'] = data['close'].rolling(window=20).std()\n",
    "    data['bb_upper'] = data['bb_middle'] + 2 * data['bb_std']\n",
    "    data['bb_lower'] = data['bb_middle'] - 2 * data['bb_std']\n",
    "    data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']\n",
    "    \n",
    "    # Volume features\n",
    "    data['volume_change'] = data['volume'].pct_change()\n",
    "    data['volume_ma_6h'] = data['volume'].rolling(window=6).mean()\n",
    "    data['volume_ma_24h'] = data['volume'].rolling(window=24).mean()\n",
    "    data['volume_ratio'] = data['volume'] / data['volume_ma_24h']\n",
    "    \n",
    "    # Time-based features (hour of day, day of week)\n",
    "    data['hour'] = data.index.hour\n",
    "    data['day_of_week'] = data.index.dayofweek\n",
    "    data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Count NaN values before dropping\n",
    "    nan_counts = data.isna().sum()\n",
    "    print(\"NaN counts per column:\")\n",
    "    print(nan_counts)\n",
    "    print(f\"Total rows with at least one NaN: {data.isna().any(axis=1).sum()}\")\n",
    "    \n",
    "    # Change to more selective NaN removal\n",
    "    # Instead of dropping all rows with any NaN, keep rows with essential data\n",
    "    essential_columns = ['close', 'volume', 'returns', 'rsi_14', 'macd']\n",
    "    data_before_dropna = data.shape[0]\n",
    "    data = data.dropna(subset=essential_columns)\n",
    "    data_after_dropna = data.shape[0]\n",
    "    print(f\"Rows before dropna: {data_before_dropna}, after dropna: {data_after_dropna}\")\n",
    "    \n",
    "    # If still losing too many rows, consider filling NaNs instead\n",
    "    if data_after_dropna < 100:  # Arbitrary threshold\n",
    "        print(\"Too many rows dropped, attempting to fill NaNs instead\")\n",
    "        data = df.copy()\n",
    "        # Apply features again but fill NaNs for rolling calculations\n",
    "        # This is a simplified example\n",
    "        data['returns'] = data['close'].pct_change().fillna(0)\n",
    "        # ... repeat other feature calculations with NaN filling ...\n",
    "    \n",
    "    print(f\"Final data shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating features...\")\n",
    "    # Load preprocessed data\n",
    "    bitcoin_data = load_preprocessed_data()\n",
    "    \n",
    "    # Add basic data inspection\n",
    "    print(f\"Loaded preprocessed data shape: {bitcoin_data.shape}\")\n",
    "    print(f\"Loaded preprocessed data columns: {bitcoin_data.columns.tolist()}\")\n",
    "    print(f\"First few rows of preprocessed data:\")\n",
    "    print(bitcoin_data.head())\n",
    "    \n",
    "    # Check for NaN values in input data\n",
    "    print(f\"NaN values in preprocessed data: {bitcoin_data.isna().sum().sum()}\")\n",
    "    \n",
    "    # Create features\n",
    "    bitcoin_features = create_features(bitcoin_data)\n",
    "    \n",
    "    # Save features\n",
    "    bitcoin_features.to_csv('data/processed/bitcoin_features.csv')\n",
    "    print(f\"Features created and saved to data/processed/bitcoin_features.csv ({len(bitcoin_features)} rows)\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17c2fc",
   "metadata": {},
   "source": [
    "### 4. Create targets\n",
    "\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_feature_data():\n",
    "    \"\"\"\n",
    "    Load Bitcoin data with features.\n",
    "    \"\"\"\n",
    "    file_path = 'data/processed/bitcoin_features.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Feature data file not found at {file_path}. Run create_features.py first.\")\n",
    "    \n",
    "    return pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "def create_targets(df, prediction_horizon=24):\n",
    "    \"\"\"\n",
    "    Create target variables for price prediction.\n",
    "    Params:\n",
    "        df: DataFrame with features\n",
    "        prediction_horizon: Number of hours to predict into the future\n",
    "    \"\"\"\n",
    "    print(f\"Input data shape: {df.shape}\")\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Future price change percentage\n",
    "    target_col = f'future_return_{prediction_horizon}h'\n",
    "    data[target_col] = data['close'].pct_change(prediction_horizon).shift(-prediction_horizon)\n",
    "    \n",
    "    # Binary target for price direction\n",
    "    direction_col = f'price_up_{prediction_horizon}h'\n",
    "    data[direction_col] = (data[target_col] > 0).astype(int)\n",
    "    \n",
    "    # Price volatility target\n",
    "    volatility_col = f'future_volatility_{prediction_horizon}h'\n",
    "    data[volatility_col] = data['returns'].rolling(window=prediction_horizon).std().shift(-prediction_horizon) * np.sqrt(prediction_horizon)\n",
    "    \n",
    "    # Print NaN counts in target columns\n",
    "    print(f\"NaN counts in target columns:\")\n",
    "    print(f\"{target_col}: {data[target_col].isna().sum()}\")\n",
    "    print(f\"{direction_col}: {data[direction_col].isna().sum()}\")\n",
    "    print(f\"{volatility_col}: {data[volatility_col].isna().sum()}\")\n",
    "    \n",
    "    # Only remove rows with NaN target values\n",
    "    rows_before = len(data)\n",
    "    data = data.dropna(subset=[target_col, direction_col, volatility_col])\n",
    "    rows_after = len(data)\n",
    "    print(f\"Rows before dropping NaN targets: {rows_before}, after: {rows_after}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating target variables...\")\n",
    "    # Load feature data\n",
    "    bitcoin_features = load_feature_data()\n",
    "    print(f\"Loaded feature data shape: {bitcoin_features.shape}\")\n",
    "    \n",
    "    # Create targets for 24-hour prediction\n",
    "    bitcoin_ml_data = create_targets(bitcoin_features, prediction_horizon=24)\n",
    "    \n",
    "    # Save ML-ready data\n",
    "    bitcoin_ml_data.to_csv('data/processed/bitcoin_ml_data.csv')\n",
    "    print(f\"Target variables created and saved to data/processed/bitcoin_ml_data.csv ({len(bitcoin_ml_data)} rows)\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3c6b8",
   "metadata": {},
   "source": [
    "### 5. K-Means\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make sure directories exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results/figures', exist_ok=True)\n",
    "\n",
    "def load_ml_data():\n",
    "    \"\"\"\n",
    "    Load ML-ready Bitcoin data.\n",
    "    \"\"\"\n",
    "    file_path = 'data/processed/bitcoin_ml_data.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"ML data file not found at {file_path}. Run create_targets.py first.\")\n",
    "    \n",
    "    return pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "def prepare_data_for_clustering(df):\n",
    "    \"\"\"\n",
    "    Prepare data for clustering.\n",
    "    \"\"\"\n",
    "    # Select features for clustering\n",
    "    cluster_features = ['returns', 'volatility_24h', 'rsi_14', 'volume_ratio', 'macd', 'bb_width']\n",
    "    X = df[cluster_features]\n",
    "    \n",
    "    # check NaN value\n",
    "    print(f\"NaN values before cleaning: {X.isna().sum().sum()}\")\n",
    "    \n",
    "    # check is there any the row of all NaN values, and deleted.\n",
    "    all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(f\"Dropping columns with all NaN values: {all_nan_cols}\")\n",
    "        X = X.drop(columns=all_nan_cols)\n",
    "        # update cluster_features table\n",
    "        cluster_features = [col for col in cluster_features if col not in all_nan_cols]\n",
    "    \n",
    "    # fill the remain NaN values.\n",
    "    X = X.fillna(X.median())\n",
    "    print(f\"NaN values after cleaning: {X.isna().sum().sum()}\")\n",
    "    \n",
    "    # Scale features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, scaler, cluster_features\n",
    "\n",
    "def perform_kmeans_clustering(X, n_clusters=4):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering on Bitcoin data.\n",
    "    \"\"\"\n",
    "    # Create and train the model\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Return cluster centers and labels\n",
    "    return kmeans, clusters\n",
    "\n",
    "def visualize_clusters(X, clusters):\n",
    "    \"\"\"\n",
    "    Visualize K-means clusters using PCA for dimensionality reduction.\n",
    "    \"\"\"\n",
    "    # Reduce dimensionality for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'PCA1': X_pca[:, 0],\n",
    "        'PCA2': X_pca[:, 1],\n",
    "        'Cluster': clusters\n",
    "    })\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=cluster_df, palette='viridis')\n",
    "    plt.title('K-means Clustering of Bitcoin Market States')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(title='Market State')\n",
    "    plt.savefig('results/figures/kmeans_clusters.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return cluster_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Performing K-means clustering...\")\n",
    "    # Load ML data\n",
    "    bitcoin_ml_data = load_ml_data()\n",
    "    \n",
    "    # Prepare data for clustering\n",
    "    X, scaler, cluster_features = prepare_data_for_clustering(bitcoin_ml_data)\n",
    "    \n",
    "    # check the dimension\n",
    "    print(f\"Clustering data shape: {X.shape}\")\n",
    "    print(f\"Features used: {cluster_features}\")\n",
    "    \n",
    "    # Perform clustering\n",
    "    kmeans_model, cluster_labels = perform_kmeans_clustering(X, n_clusters=4)\n",
    "    \n",
    "    # Visualize clusters\n",
    "    cluster_df = visualize_clusters(X, cluster_labels)\n",
    "    \n",
    "    # Add cluster labels to original data\n",
    "    bitcoin_ml_data['cluster'] = cluster_labels\n",
    "    bitcoin_ml_data[['cluster']].to_csv('data/processed/bitcoin_clusters.csv')\n",
    "    \n",
    "    # Save model and scaler\n",
    "    joblib.dump(kmeans_model, 'models/kmeans_model.pkl')\n",
    "    joblib.dump(scaler, 'models/kmeans_scaler.pkl')\n",
    "    joblib.dump(cluster_features, 'models/kmeans_features.pkl')\n",
    "    \n",
    "    print(\"K-means clustering performed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ea722",
   "metadata": {},
   "source": [
    "### 6. Random Forest\n",
    "\n",
    "```Python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make sure directories exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results/figures', exist_ok=True)\n",
    "os.makedirs('results/metrics', exist_ok=True)\n",
    "\n",
    "def load_ml_data():\n",
    "    \"\"\"\n",
    "    Load ML-ready Bitcoin data.\n",
    "    \"\"\"\n",
    "    file_path = 'data/processed/bitcoin_ml_data.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"ML data file not found at {file_path}. Run create_targets.py first.\")\n",
    "    \n",
    "    return pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "def prepare_data(df, target_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for machine learning models.\n",
    "    \"\"\"\n",
    "    # Define features and target\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col, 'open', 'high', 'low', 'close', 'volume']]\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_cols\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest regression model.\n",
    "    \"\"\"\n",
    "    # Create and train the model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Random Forest MSE: {mse:.6f}\")\n",
    "    print(f\"Random Forest R²: {r2:.6f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(15))\n",
    "    plt.title('Top 15 Feature Importance in Random Forest Model')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/figures/rf_feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = pd.DataFrame({\n",
    "        'model': ['Random Forest'],\n",
    "        'mse': [mse],\n",
    "        'r2': [r2]\n",
    "    })\n",
    "    metrics.to_csv('results/metrics/rf_performance.csv', index=False)\n",
    "    \n",
    "    return rf_model, y_pred, feature_importance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training Random Forest model...\")\n",
    "    # Load ML data\n",
    "    bitcoin_ml_data = load_ml_data()\n",
    "    \n",
    "    # Prepare data for price prediction\n",
    "    X_train, X_test, y_train, y_test, scaler, feature_cols = prepare_data(\n",
    "        bitcoin_ml_data, 'future_return_24h', test_size=0.2\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate Random Forest model\n",
    "    rf_model, rf_pred, rf_importance = train_random_forest(X_train, y_train, X_test, y_test, feature_cols)\n",
    "    \n",
    "    # Save model and scaler\n",
    "    joblib.dump(rf_model, 'models/random_forest_model.pkl')\n",
    "    joblib.dump(scaler, 'models/rf_scaler.pkl')\n",
    "    joblib.dump(feature_cols, 'models/feature_cols.pkl')\n",
    "    \n",
    "    print(\"Random Forest model trained and saved.\")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fa99c",
   "metadata": {},
   "source": [
    "### 7. SVR\n",
    "\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Make sure directories exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results/metrics', exist_ok=True)\n",
    "\n",
    "def load_ml_data():\n",
    "    \"\"\"\n",
    "    Load ML-ready Bitcoin data.\n",
    "    \"\"\"\n",
    "    file_path = 'data/processed/bitcoin_ml_data.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"ML data file not found at {file_path}. Run create_targets.py first.\")\n",
    "    \n",
    "    return pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "def prepare_data(df, target_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for machine learning models.\n",
    "    \"\"\"\n",
    "    # Define features and target\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col, 'open', 'high', 'low', 'close', 'volume']]\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Check for NaN values\n",
    "    print(f\"NaN values in features before cleaning: {X.isna().sum().sum()}\")\n",
    "    \n",
    "    # check the NaN raw, and deleted.\n",
    "    all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(f\"Dropping columns with all NaN values: {all_nan_cols}\")\n",
    "        X = X.drop(columns=all_nan_cols)\n",
    "    \n",
    "    # using median to filled the NaN value.\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    print(f\"NaN values in features after cleaning: {X.isna().sum().sum()}\")\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    # Scale features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, X.columns.tolist()\n",
    "\n",
    "def train_svr(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Support Vector Regression model.\n",
    "    \"\"\"\n",
    "    # Create and train the model\n",
    "    svr_model = SVR(kernel='rbf', C=10, epsilon=0.1, gamma='scale')\n",
    "    svr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = svr_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"SVR MSE: {mse:.6f}\")\n",
    "    print(f\"SVR R²: {r2:.6f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = pd.DataFrame({\n",
    "        'model': ['SVR'],\n",
    "        'mse': [mse],\n",
    "        'r2': [r2]\n",
    "    })\n",
    "    metrics.to_csv('results/metrics/svr_performance.csv', index=False)\n",
    "    \n",
    "    return svr_model, y_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training SVR model...\")\n",
    "    # Load ML data\n",
    "    bitcoin_ml_data = load_ml_data()\n",
    "    \n",
    "    # Prepare data for price prediction\n",
    "    X_train, X_test, y_train, y_test, scaler, feature_cols = prepare_data(\n",
    "        bitcoin_ml_data, 'future_return_24h', test_size=0.2\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate SVR model\n",
    "    svr_model, svr_pred = train_svr(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Save model and scaler\n",
    "    joblib.dump(svr_model, 'models/svr_model.pkl')\n",
    "    joblib.dump(scaler, 'models/svr_scaler.pkl')\n",
    "    joblib.dump(feature_cols, 'models/svr_features.pkl')\n",
    "    \n",
    "    print(\"SVR model trained and saved.\")\n",
    "    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4a391",
   "metadata": {},
   "source": [
    "### 8. Plot cluster\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_clusters():\n",
    "    \"\"\"\n",
    "    Visualize K-means clustering results on Bitcoin data.\n",
    "    \"\"\"\n",
    "    print(\"Visualizing cluster results...\")\n",
    "    \n",
    "    # Ensure directories exist\n",
    "    os.makedirs('results/figures', exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Check if the clustered data file exists\n",
    "        cluster_file = 'data/processed/bitcoin_clusters.csv'\n",
    "        if not os.path.exists(cluster_file):\n",
    "            print(f\"Cluster data file not found: {cluster_file}\")\n",
    "            \n",
    "            # Look for alternative file\n",
    "            ml_data_file = 'data/processed/bitcoin_ml_data.csv'\n",
    "            if os.path.exists(ml_data_file):\n",
    "                print(f\"Loading ML data from: {ml_data_file}\")\n",
    "                data = pd.read_csv(ml_data_file, index_col=0, parse_dates=True)\n",
    "                \n",
    "                # Check if kmeans model exists\n",
    "                model_files = os.listdir('models')\n",
    "                kmeans_model_path = None\n",
    "                \n",
    "                for file in model_files:\n",
    "                    if 'kmeans' in file.lower() and 'model' in file.lower():\n",
    "                        kmeans_model_path = os.path.join('models', file)\n",
    "                        break\n",
    "                \n",
    "                if kmeans_model_path and os.path.exists(kmeans_model_path):\n",
    "                    print(f\"Loading K-means model from: {kmeans_model_path}\")\n",
    "                    kmeans_model = joblib.load(kmeans_model_path)\n",
    "                    \n",
    "                    # Prepare features for clustering\n",
    "                    feature_cols = ['returns', 'volatility_24h', 'rsi_14', 'volume_ratio', 'macd', 'bb_width']\n",
    "                    valid_features = [col for col in feature_cols if col in data.columns]\n",
    "                    \n",
    "                    X = data[valid_features]\n",
    "                    X = X.fillna(X.median())\n",
    "                    \n",
    "                    # Apply clustering\n",
    "                    clusters = kmeans_model.predict(X)\n",
    "                    data['cluster'] = clusters\n",
    "                else:\n",
    "                    print(\"K-means model not found. Creating random clusters for visualization.\")\n",
    "                    # Create random clusters for demonstration\n",
    "                    np.random.seed(42)\n",
    "                    data['cluster'] = np.random.randint(0, 4, size=len(data))\n",
    "            else:\n",
    "                print(\"No data files found for clustering visualization.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Loading cluster data from: {cluster_file}\")\n",
    "            data = pd.read_csv(cluster_file, index_col=0, parse_dates=True)\n",
    "            \n",
    "            # If only cluster column is present, load full data\n",
    "            if len(data.columns) <= 1:\n",
    "                print(\"Loading full data for visualization...\")\n",
    "                ml_data = pd.read_csv('data/processed/bitcoin_ml_data.csv', index_col=0, parse_dates=True)\n",
    "                data = pd.concat([ml_data, data], axis=1)\n",
    "        \n",
    "        # 1. Plot time series with cluster colors\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        scatter = plt.scatter(data.index, data['close'], c=data['cluster'], \n",
    "                              cmap='viridis', alpha=0.7, s=40)\n",
    "        \n",
    "        plt.title('Bitcoin Price with Cluster Classifications')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price (USD)')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/figures/cluster_time_series.png')\n",
    "                \n",
    "\n",
    "        # 2. Perform PCA for visualization\n",
    "        # Select numerical columns for PCA\n",
    "        num_cols = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        num_cols = [col for col in num_cols if col != 'cluster']\n",
    "\n",
    "        if len(num_cols) > 2:\n",
    "            # Select a subset of columns that have minimal NaN values\n",
    "            X = data[num_cols].copy()\n",
    "            \n",
    "            # Check which columns have NaN values\n",
    "            nan_count = X.isna().sum()\n",
    "            good_cols = nan_count[nan_count < len(X) * 0.1].index.tolist()  # Columns with <10% NaNs\n",
    "            \n",
    "            if len(good_cols) < 2:\n",
    "                print(\"Not enough good columns for PCA. Using the least NaN columns.\")\n",
    "                # Get columns with the least NaNs\n",
    "                good_cols = nan_count.nsmallest(min(5, len(nan_count))).index.tolist()\n",
    "            \n",
    "            print(f\"Using {len(good_cols)} columns for PCA: {good_cols}\")\n",
    "            X = X[good_cols]\n",
    "            \n",
    "            # Handle any remaining NaN values\n",
    "            X = X.fillna(X.median())\n",
    "            \n",
    "            # Apply PCA\n",
    "            try:\n",
    "                pca = PCA(n_components=2)\n",
    "                X_pca = pca.fit_transform(X)\n",
    "                \n",
    "                # Create DataFrame for plotting\n",
    "                pca_df = pd.DataFrame({\n",
    "                    'PC1': X_pca[:, 0],\n",
    "                    'PC2': X_pca[:, 1],\n",
    "                    'Cluster': data.loc[X.index, 'cluster']\n",
    "                })\n",
    "                \n",
    "                # Plot PCA results\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='viridis', s=60)\n",
    "                plt.title('PCA Visualization of Bitcoin Market States')\n",
    "                plt.xlabel(f'PC1 (variance)')\n",
    "                plt.ylabel(f'PC2 (variance)')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('results/figures/cluster_pca.png')\n",
    "            except Exception as e:\n",
    "                print(f\"Error during PCA visualization: {str(e)}\")\n",
    "        # 3. Plot cluster statistics\n",
    "        # Calculate key statistics by cluster\n",
    "        cluster_stats = data.groupby('cluster').agg({\n",
    "            'future_return_24h': ['mean', 'std'],\n",
    "            'returns': ['mean', 'std'],\n",
    "            'volume': 'mean',\n",
    "            'rsi_14': 'mean'\n",
    "        })\n",
    "        \n",
    "        # Plot return by cluster\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot average future return by cluster\n",
    "        plt.subplot(2, 2, 1)\n",
    "        returns_by_cluster = cluster_stats['future_return_24h']['mean']\n",
    "        plt.bar(returns_by_cluster.index, returns_by_cluster.values)\n",
    "        plt.title('Average 24h Future Return by Cluster')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Avg. Return (%)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot return volatility by cluster\n",
    "        plt.subplot(2, 2, 2)\n",
    "        volatility_by_cluster = cluster_stats['future_return_24h']['std']\n",
    "        plt.bar(volatility_by_cluster.index, volatility_by_cluster.values)\n",
    "        plt.title('Return Volatility by Cluster')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Return Std. Dev.')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot average RSI by cluster\n",
    "        plt.subplot(2, 2, 3)\n",
    "        if 'rsi_14' in cluster_stats.columns.get_level_values(0):\n",
    "            rsi_by_cluster = cluster_stats['rsi_14']['mean']\n",
    "            plt.bar(rsi_by_cluster.index, rsi_by_cluster.values)\n",
    "            plt.title('Average RSI by Cluster')\n",
    "            plt.xlabel('Cluster')\n",
    "            plt.ylabel('Avg. RSI')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot volume by cluster\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if 'volume' in cluster_stats.columns.get_level_values(0):\n",
    "            volume_by_cluster = cluster_stats['volume']['mean']\n",
    "            plt.bar(volume_by_cluster.index, volume_by_cluster.values)\n",
    "            plt.title('Average Volume by Cluster')\n",
    "            plt.xlabel('Cluster')\n",
    "            plt.ylabel('Avg. Volume')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/figures/cluster_statistics.png')\n",
    "        \n",
    "        print(\"Cluster visualizations saved to results/figures/\")\n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing clusters: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_clusters()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfed93",
   "metadata": {},
   "source": [
    "### 9. Plot prediction\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_predictions():\n",
    "    \"\"\"\n",
    "    Plot model predictions against actual values.\n",
    "    \"\"\"\n",
    "    print(\"Plotting model predictions...\")\n",
    "    \n",
    "    # Ensure directories exist\n",
    "    os.makedirs('results/figures', exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load ML data\n",
    "        ml_data = pd.read_csv('data/processed/bitcoin_ml_data.csv', index_col=0, parse_dates=True)\n",
    "        \n",
    "        # Target variable\n",
    "        target_col = 'future_return_24h'\n",
    "        \n",
    "        # Get features for prediction\n",
    "        feature_cols = [col for col in ml_data.columns if col not in [target_col, 'open', 'high', 'low', 'close', 'volume', \n",
    "                                                              'price_up_24h', 'future_volatility_24h']]\n",
    "        \n",
    "        # Prepare data\n",
    "        X = ml_data[feature_cols]\n",
    "        y = ml_data[target_col]\n",
    "        \n",
    "        # Check for all-NaN columns\n",
    "        all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "        if all_nan_cols:\n",
    "            print(f\"Dropping columns with all NaN values: {all_nan_cols}\")\n",
    "            X = X.drop(columns=all_nan_cols)\n",
    "        \n",
    "        # Handle remaining NaN values\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        # Split data for visualization\n",
    "        test_size = 0.2\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "        \n",
    "        # Load models\n",
    "        models = {}\n",
    "        scalers = {}\n",
    "        \n",
    "        # Check available model files\n",
    "        model_files = os.listdir('models')\n",
    "        print(f\"Available model files: {model_files}\")\n",
    "        \n",
    "        # Try to load Random Forest model\n",
    "        rf_model_path = None\n",
    "        for file in model_files:\n",
    "            if 'random_forest' in file.lower() and 'model' in file.lower():\n",
    "                rf_model_path = os.path.join('models', file)\n",
    "                break\n",
    "        \n",
    "        if rf_model_path and os.path.exists(rf_model_path):\n",
    "            print(f\"Loading Random Forest model from: {rf_model_path}\")\n",
    "            models['Random Forest'] = joblib.load(rf_model_path)\n",
    "        \n",
    "        # Try to load SVR model\n",
    "        svr_model_path = None\n",
    "        for file in model_files:\n",
    "            if 'svr' in file.lower() and 'model' in file.lower():\n",
    "                svr_model_path = os.path.join('models', file)\n",
    "                break\n",
    "        \n",
    "        if svr_model_path and os.path.exists(svr_model_path):\n",
    "            print(f\"Loading SVR model from: {svr_model_path}\")\n",
    "            models['SVR'] = joblib.load(svr_model_path)\n",
    "        \n",
    "        # Create a scaler\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = {}\n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                # Get the expected number of features for this model\n",
    "                expected_features = None\n",
    "                try:\n",
    "                    expected_features = model.n_features_in_\n",
    "                    print(f\"{name} model expects {expected_features} features, we have {X_test_scaled.shape[1]} features\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try to make prediction\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                predictions[name] = y_pred\n",
    "            except Exception as e:\n",
    "                print(f\"Error making predictions with {name} model: {str(e)}\")\n",
    "                # Try with a retrained model\n",
    "                print(f\"Retraining {name} model with current features...\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                predictions[name] = y_pred\n",
    "        \n",
    "        # Create DataFrame for plotting\n",
    "        plot_data = pd.DataFrame({'Actual': y_test})\n",
    "        for name, pred in predictions.items():\n",
    "            plot_data[name] = pred\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(plot_data.index, plot_data['Actual'], label='Actual', linewidth=2)\n",
    "        \n",
    "        for name in predictions.keys():\n",
    "            plt.plot(plot_data.index, plot_data[name], label=f'{name} Prediction', linewidth=1.5, alpha=0.8)\n",
    "        \n",
    "        plt.title('Bitcoin Price Return Predictions')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Return (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/figures/model_predictions.png')\n",
    "        \n",
    "        # Plot error distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, (name, pred) in enumerate(predictions.items(), 1):\n",
    "            errors = plot_data['Actual'] - plot_data[name]\n",
    "            \n",
    "            plt.subplot(1, len(predictions), i)\n",
    "            sns.histplot(errors, kde=True)\n",
    "            plt.title(f'{name} Error Distribution')\n",
    "            plt.xlabel('Prediction Error')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/figures/error_distribution.png')\n",
    "        \n",
    "        # Plot scatter of actual vs predicted\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, (name, pred) in enumerate(predictions.items(), 1):\n",
    "            plt.subplot(1, len(predictions), i)\n",
    "            plt.scatter(plot_data['Actual'], plot_data[name], alpha=0.5)\n",
    "            \n",
    "            # Add perfect prediction line\n",
    "            min_val = min(plot_data['Actual'].min(), plot_data[name].min())\n",
    "            max_val = max(plot_data['Actual'].max(), plot_data[name].max())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "            \n",
    "            plt.title(f'Actual vs {name} Predicted')\n",
    "            plt.xlabel('Actual Return')\n",
    "            plt.ylabel('Predicted Return')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/figures/actual_vs_predicted.png')\n",
    "        \n",
    "        print(\"Model prediction plots saved to results/figures/\")\n",
    "        return plot_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting predictions: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_predictions()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a199d67",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
