{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7f52fb",
   "metadata": {},
   "source": [
    "# Bitcoin Price Analysis and Prediction Using Machine Learning\n",
    "\n",
    "## Web link to dataset\n",
    "[GitHub Repository](https://github.com/chu-siang/Bitcoin_Analysis_ML/tree/main)\n",
    "\n",
    "[GitHub Repository (raw data)](https://github.com/chu-siang/Bitcoin_Analysis_ML/blob/main/data/raw/bitcoin_raw_data.csv)\n",
    "\n",
    "[GitHub Repository (preprocessed data)](https://github.com/chu-siang/Bitcoin_Analysis_ML/blob/main/data/processed/bitcoin_preprocessed.csv)\n",
    "\n",
    "[GitHub Repository (clusters data)](https://github.com/chu-siang/Bitcoin_Analysis_ML/blob/main/data/processed/bitcoin_clusters.csv)\n",
    "\n",
    "[GitHub Repository (features data)](https://github.com/chu-siang/Bitcoin_Analysis_ML/blob/main/data/processed/bitcoin_features.csv)\n",
    "\n",
    "## Research Question\n",
    "This research investigates how effectively machine learning models can predict Bitcoin price movements based on historical price data and technical indicators using 1-hour intervals over a 4-month period (November 2024 to March 2025). Additionally, we explore whether unsupervised learning can identify distinct market states with different behavior patterns.\n",
    "\n",
    "## Dataset Documentation\n",
    "\n",
    "### Data Type and External Source\n",
    "The dataset consists of Bitcoin (BTC/USDT) price data collected from the Binance API, including 1-hour candlestick information from November 1, 2024, to March 1, 2025. The raw data includes time-series records of Bitcoin's price and trading volume, with each record containing a timestamp and OHLCV (Open, High, Low, Close, Volume) values.\n",
    "\n",
    "### Dataset Preprocessing and Feature Engineering\n",
    "I conducted several preprocessing steps to ensure data quality:\n",
    "- Converting timestamps to datetime format\n",
    "- Ensuring consistent hourly intervals\n",
    "- Removing missing timestamps and sorting chronologically\n",
    "- Creating technical indicators as features\n",
    "- Generating target variables for prediction\n",
    "\n",
    "From the raw data, I derived over 30 technical indicators and features including:\n",
    "- Simple Moving Averages (SMA): 12-hour SMA for smoothing short-term fluctuations\n",
    "- Exponential Moving Averages (EMA): 6h, 12h, 24h EMAs emphasizing recent price trends\n",
    "- Relative Strength Index (RSI): 14-period RSI signaling overbought (>70) or oversold (<30) conditions\n",
    "- Moving Average Convergence Divergence (MACD): Trend momentum indicators\n",
    "- Bollinger Bands: For volatility and extreme price deviation detection\n",
    "- Volatility measures: 24-hour volatility and volume-related features\n",
    "- Time-based features: Hour of day (0-23) and weekend flags (1 for Saturday/Sunday, 0 otherwise)\n",
    "\n",
    "### Target Variable Construction\n",
    "For supervised learning, we created the following targets:\n",
    "- 24-hour future return (return_24h): Percentage change from current time (t) to 24 hours later (t+24h)\n",
    "- Binary price direction (price_up_24h): 1 if future return positive, 0 otherwise\n",
    "- Future volatility (future_volatility_24h): For cluster analysis only, not prediction\n",
    "\n",
    "![Bitcoin Price with Cluster Classifications](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/cluster_time_series.png)\n",
    "*Figure 1: Bitcoin price chart with identified market state clusters. Each point represents Bitcoin price colored by its assigned cluster, showing how market states evolve over time from November 2024 to March 2025. Purple points (Cluster 0) appear during sideways movement, green points (Cluster 2) during uptrends, blue points (Cluster 1) during corrections, and yellow points (Cluster 3) often after price drops.*\n",
    "\n",
    "## Description of Supervised and Unsupervised Methods\n",
    "\n",
    "### Supervised Learning Methods\n",
    "\n",
    "#### 1. Random Forest Regression\n",
    "Random Forest is an ensemble learning method that constructs multiple decision trees and outputs the average prediction to improve accuracy and control overfitting.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Framework: scikit-learn's RandomForestRegressor\n",
    "- Features: 30+ technical indicators\n",
    "- Target: 24-hour future returns\n",
    "- Hyperparameters: 100 trees, default depth\n",
    "\n",
    "![Random Forest Feature Importance](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/rf_feature_importance.png)\n",
    "*Figure 2: Top 15 feature importances in the Random Forest model. The chart shows which features most influenced predictions, with price_up_24h and future_volatility_24h having the highest importance scores (indicating some target leakage). Among legitimate technical indicators, Bollinger Bands (bb_upper) and moving averages (sma_24h) contributed the most to predictions.*\n",
    "\n",
    "#### 2. Support Vector Regression (SVR)\n",
    "SVR finds a function that best predicts the continuous output value for a given input value, while maximizing the margin.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Framework: scikit-learn's SVR\n",
    "- Features: Same technical indicators used for Random Forest\n",
    "- Target: 24-hour future returns\n",
    "- Hyperparameters: RBF kernel, C=10, epsilon=0.1\n",
    "\n",
    "### Unsupervised Learning Method\n",
    "\n",
    "#### K-means Clustering\n",
    "K-means clustering was applied to identify distinct market states or regimes in Bitcoin trading patterns by grouping data into clusters that minimize intra-cluster variance.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Framework: scikit-learn's KMeans\n",
    "- Features: Selected subset including returns, volatility, RSI, volume ratio, MACD, Bollinger Band width\n",
    "- Parameters: 4 clusters (k=4)\n",
    "- No future/target information was included in clustering\n",
    "\n",
    "![K-means Clustering Visualization](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/kmeans_clusters.png)\n",
    "*Figure 3: K-means clustering of Bitcoin market states visualized in principal component space. Each point represents an hourly observation projected onto two principal components and colored by cluster. The visualization shows clear separation between the four market states: purple points (Cluster 0) forming a dense group at bottom-left, green points (Cluster 2) spreading to the right, blue points (Cluster 1) in the upper-middle region, and yellow points (Cluster 3) appearing in the upper-left.*\n",
    "\n",
    "## Description of Experiments and Evaluation Results\n",
    "\n",
    "### Experiment 1: Regression Performance and Cross-Validation\n",
    "I compared the performance of Random Forest and SVR models in predicting 24-hour Bitcoin price returns, using chronological train-test splitting and 5-fold cross-validation.\n",
    "\n",
    "**Results:**\n",
    "The Random Forest significantly outperformed SVR, achieving lower MSE and better R² values. However, both models had negative R² scores on the test set, indicating limited predictive power for the highly volatile Bitcoin returns. The RF partially captured directional movements, while SVR predicted mostly near-zero returns.\n",
    "\n",
    "![Model Prediction Comparison](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/actual_vs_predicted.png)\n",
    "*Figure 4: Scatter plots of actual vs. predicted 24h returns for Random Forest (left) and SVR (right). The red dashed line represents perfect prediction. The Random Forest shows predictions somewhat correlated with actual returns, while SVR predictions cluster horizontally near zero, indicating its failure to capture return variability.*\n",
    "\n",
    "![Error Distribution](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/error_distribution.png)\n",
    "*Figure 5: Histograms showing prediction error distributions for Random Forest (left) and SVR (right). The RF errors are more tightly centered around zero with fewer extreme errors, while SVR shows a broader, skewed distribution, confirming its tendency to underpredict returns.*\n",
    "\n",
    "![Bitcoin Price Return Predictions](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/model_predictions.png)\n",
    "*Figure 6: Time series of actual Bitcoin returns (blue) compared with Random Forest predictions (orange) and SVR predictions (green) during February 2025. The RF partially tracks actual return patterns but underestimates extreme movements, while SVR predictions remain near zero throughout, demonstrating its limited predictive power.*\n",
    "\n",
    "### Experiment 2: Data Augmentation Effects\n",
    "I investigated four augmentation techniques beyond the baseline (no augmentation):\n",
    "- Gaussian noise addition (std=0.01, 0.05, 0.1)\n",
    "- Synthetic sample mixing (averaging random pairs of training examples)\n",
    "\n",
    "**Results:**\n",
    "Moderate Gaussian noise (std=0.05) yielded the best improvement, reducing MSE from 0.00075 to 0.00071 and improving R² from -0.8 to -0.7. This suggests that adding controlled noise can act as a regularizer, helping the model generalize better. Smaller noise levels were ineffective, while synthetic mixing did not help.\n",
    "\n",
    "![Data Augmentation Results](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/experiments/data_augmentation_experiment.png)\n",
    "*Figure 7: Bar charts comparing MSE (left) and R² (right) for different data augmentation methods. The baseline shows moderate performance, tiny noise (std=0.01) slightly worsens results, moderate noise (std=0.05) gives the best performance with lowest MSE and highest R², while higher noise (std=0.1) and synthetic mixing show results similar to baseline.*\n",
    "\n",
    "### Experiment 3: PCA Dimensionality Reduction\n",
    "I tested how using principal component analysis (PCA) to reduce feature dimensions affected model performance.\n",
    "\n",
    "**Results:**\n",
    "PCA dimensionality reduction degraded model performance. Using fewer components (e.g., 3) significantly increased MSE and worsened R². Performance gradually improved as more components were added, approaching but not exceeding the original feature set performance. This indicates that the full feature set contains valuable information not captured by the first few principal components.\n",
    "\n",
    "![PCA Experiment Results](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/experiments/pca_experiment.png)\n",
    "*Figure 8: Effects of PCA dimensionality reduction on model performance. The top-left chart shows MSE increasing dramatically with fewer components and gradually improving as components increase. Top-right shows R² following the opposite pattern. Bottom-left relates MSE to variance explained, and bottom-right displays the component loadings heatmap showing how original features contribute to principal components.*\n",
    "\n",
    "### Experiment 4: Market State Clustering\n",
    "I analyzed the four market states identified by K-means clustering to understand their characteristics and temporal distribution.\n",
    "\n",
    "**Results:**\n",
    "The clustering revealed four distinct market regimes with clear differences in future returns, volatility, RSI, and volume:\n",
    "\n",
    "- **Cluster 0 (Purple)**: \"Calm Market\" - Low returns (~0.05%), neutral RSI (~50), lowest volatility (~0.022) and volume (~800). Represents sideways, low-activity periods.\n",
    "\n",
    "- **Cluster 1 (Blue)**: \"Correction Phase\" - Moderate returns (~0.25%), relatively low RSI (~35-40), high volatility (~0.028), moderate volume (~1500). Represents recovering or dipping markets.\n",
    "\n",
    "- **Cluster 2 (Green)**: \"Bull Market\" - High returns (~0.30%), very high RSI (~70), highest volatility (~0.030), high volume (~2500). Represents bullish trending states.\n",
    "\n",
    "- **Cluster 3 (Yellow)**: \"Reversal State\" - Highest returns (~0.35%), lowest RSI (~30), high volatility (~0.028), highest volume (~3200). Represents post-crash rebound situations.\n",
    "\n",
    "![PCA Visualization of Clusters](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/cluster_pca.png)\n",
    "*Figure 9: Alternative PCA visualization of the Bitcoin market state clusters. This projection shows the distinctive distribution of cluster points across the principal component space with different variance scaling, highlighting outlier points and revealing the large-scale structure of the dataset.*\n",
    "\n",
    "![Cluster Characteristics](https://raw.githubusercontent.com/chu-siang/Bitcoin_Analysis_ML/refs/heads/main/results/figures/cluster_statistics.png)\n",
    "*Figure 10: Bar charts showing key statistical properties of each cluster. Top-left shows average 24h future returns (highest in Cluster 3); top-right shows return volatility (lowest in Cluster 0); bottom-left shows average RSI (highest in Cluster 2); bottom-right shows trading volume (highest in Cluster 3). These metrics reveal the distinct nature of each market state.*\n",
    "\n",
    "The temporal distribution of clusters aligned with observable market behavior. Green cluster points appeared during strong uptrends, yellow cluster points often followed local price minima, blue cluster points appeared during corrections, and purple cluster points predominated during flat periods.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Predictive Challenge**: Both supervised models struggled to predict exact 24-hour returns (negative R²), with Random Forest performing better than SVR. This aligns with the efficient market hypothesis that short-term price movements are difficult to predict.\n",
    "\n",
    "2. **Technical Indicators**: No single technical indicator strongly predicted future returns. The model relied on combinations of features with Bollinger Bands and moving averages showing modest predictive value.\n",
    "\n",
    "3. **Data Augmentation**: Adding moderate Gaussian noise (5-10% of feature scale) during training improved model performance slightly, suggesting it helps mitigate overfitting.\n",
    "\n",
    "4. **Feature Dimensionality**: PCA dimensionality reduction decreased performance, indicating the model benefits from the full feature space and complex feature interactions.\n",
    "\n",
    "5. **Market State Identification**: Unsupervised clustering successfully identified four meaningful market regimes: calm sideways markets, corrections, bullish trends, and post-crash reversals. These clusters showed distinct characteristics in returns, volatility, RSI, and volume.\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "1. **Advanced Models**: Test deep learning approaches like LSTM networks that might better capture temporal dependencies in price data.\n",
    "\n",
    "2. **Additional Data**: Incorporate sentiment analysis, on-chain metrics, or macroeconomic indicators to provide broader context.\n",
    "\n",
    "3. **Alternative Approaches**: Reframe the prediction task as classification rather than regression to potentially achieve better results.\n",
    "\n",
    "4. **Time Sequence Modeling**: Explore sequence models that account for autocorrelation in returns and indicators over time.\n",
    "\n",
    "5. **Practical Applications**: While direct return prediction remains challenging, the market state clustering offers practical value for risk management and trading strategy adaptation.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Random forests. Machine Learning :  https://medium.com/chung-yi/ml%E5%85%A5%E9%96%80-%E5%8D%81%E4%B8%83-%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-6afc24871857\n",
    "2. Support-vector networks. Machine Learning. https://scikit-learn.org/stable/modules/svm.html \n",
    "3. Scikit-learn: Machine Learning in Python. https://scikit-learn.org/stable/\n",
    "4. pandas: a Foundational Python Library for Data Analysis. https://pandas.pydata.org/ \n",
    "5. Binance API Documentation. https://binance-docs.github.io/apidocs/\n",
    "\n",
    "## Appendix: Project Structure\n",
    "\n",
    "The repository contains the following key files and directories:\n",
    "\n",
    "```\n",
    "bitcoin-analysis/\n",
    "├── README.md\n",
    "├── requirements.txt\n",
    "├── Makefile\n",
    "├── data/\n",
    "│   ├── raw/\n",
    "│   │   └── bitcoin_raw_data.csv\n",
    "│   └── processed/\n",
    "│       ├── bitcoin_features.csv\n",
    "│       └── bitcoin_ml_data.csv\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── data/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── fetch_data.py\n",
    "│   │   └── preprocess.py\n",
    "│   ├── features/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── create_features.py\n",
    "│   │   └── create_targets.py\n",
    "│   ├── models/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── random_forest.py\n",
    "│   │   ├── svr.py\n",
    "│   │   └── kmeans.py\n",
    "│   └── visualization/\n",
    "│       ├── __init__.py\n",
    "│       ├── plot_predictions.py\n",
    "│       ├── plot_clusters.py\n",
    "│       └── plot_experiments.py\n",
    "├── experiments/\n",
    "│   ├── __init__.py\n",
    "│   ├── data_size.py\n",
    "│   ├── data_augmentation.py\n",
    "│   └── dimensionality_reduction.py\n",
    "├── models/\n",
    "│   ├── random_forest_model.pkl\n",
    "│   ├── svr_model.pkl\n",
    "│   ├── kmeans_model.pkl\n",
    "│   ├── optimized_random_forest_model.pkl\n",
    "│   └── optimized_svr_model.pkl\n",
    "├── results/\n",
    "│   ├── figures/\n",
    "│   │   ├── prediction_comparison.png\n",
    "│   │   ├── feature_importance.png\n",
    "│   │   ├── kmeans_clusters.png\n",
    "│   │   ├── training_size_experiment.png\n",
    "│   │   ├── augmentation_experiment.png\n",
    "│   │   ├── pca_experiment.png\n",
    "│   │   └── cluster_analysis.png\n",
    "│   └── metrics/\n",
    "│       └── model_performance.csv\n",
    "└── report/\n",
    "    ├── bitcoin_analysis_report.pdf\n",
    "    └── figures/\n",
    "        ├── price_chart.png\n",
    "        ├── model_comparison.png\n",
    "        └── cluster_visualization.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34036655",
   "metadata": {},
   "source": [
    "## Appendix: Code PART !!!\n",
    "\n",
    "### 1. Fetch Data\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# Make sure data directories exist\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "def fetch_bitcoin_data(start_date, end_date, interval='1h'):\n",
    "    \"\"\"\n",
    "    Fetch Bitcoin price data from a public API.\n",
    "    Params:\n",
    "        start_date: Start date in YYYY-MM-DD format\n",
    "        end_date: End date in YYYY-MM-DD format\n",
    "        interval: Data granularity (1h for 1-hour data)\n",
    "    Returns:\n",
    "        DataFrame with OHLCV data\n",
    "    \"\"\"\n",
    "    # Convert dates to timestamps\n",
    "    start_ts = int(datetime.datetime.strptime(start_date, '%Y-%m-%d').timestamp() * 1000)\n",
    "    end_ts = int(datetime.datetime.strptime(end_date, '%Y-%m-%d').timestamp() * 1000)\n",
    "    \n",
    "    # Binance API endpoint for historical klines (candlestick) data\n",
    "    url = 'https://api.binance.com/api/v3/klines'\n",
    "    \n",
    "    # Parameters for API request\n",
    "    params = {\n",
    "        'symbol': 'BTCUSDT',\n",
    "        'interval': interval,\n",
    "        'startTime': start_ts,\n",
    "        'endTime': end_ts,\n",
    "        'limit': 1000  # Max limit per request\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Fetch data in chunks if needed\n",
    "    while start_ts < end_ts:\n",
    "        params['startTime'] = start_ts\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            break\n",
    "            \n",
    "        all_data.extend(data)\n",
    "        \n",
    "        # Update start_ts for next iteration\n",
    "        start_ts = data[-1][0] + 1\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', \n",
    "                                         'close_time', 'quote_asset_volume', 'trades', \n",
    "                                         'taker_buy_base', 'taker_buy_quote', 'ignored'])\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    \n",
    "    # Convert price columns to float\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        df[col] = df[col].astype(float)\n",
    "    \n",
    "    # Set timestamp as index\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    return df[['open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Fetching Bitcoin price data...\")\n",
    "    # Fetch 4 months of 1-hour Bitcoin data\n",
    "    start_date = '2024-11-01'\n",
    "    end_date = '2025-03-01'\n",
    "    bitcoin_data = fetch_bitcoin_data(start_date, end_date)\n",
    "    \n",
    "    # Save the raw data\n",
    "    bitcoin_data.to_csv('data/raw/bitcoin_raw_data.csv')\n",
    "    print(f\"Raw data saved to data/raw/bitcoin_raw_data.csv ({len(bitcoin_data)} rows)\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79f735",
   "metadata": {},
   "source": [
    "### 2. Preprocess Data\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"\n",
    "    Load raw Bitcoin price data.\n",
    "    \"\"\"\n",
    "    file_path = 'data/raw/bitcoin_raw_data.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Raw data file not found at {file_path}. Run fetch_data.py first.\")\n",
    "    \n",
    "    return pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preprocessing raw data...\")\n",
    "    # Load raw data\n",
    "    bitcoin_data = load_raw_data()\n",
    "    \n",
    "    # Perform basic preprocessing (if needed)\n",
    "    # Remove duplicate rows\n",
    "    bitcoin_data = bitcoin_data.drop_duplicates()\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    bitcoin_data = bitcoin_data.sort_index()\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    bitcoin_data.to_csv('data/processed/bitcoin_preprocessed.csv')\n",
    "    print(f\"Preprocessed data saved to data/processed/bitcoin_preprocessed.csv ({len(bitcoin_data)} rows)\")\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bfc93",
   "metadata": {},
   "source": [
    "### 3. Create Features\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    \"\"\"\n",
    "    Load preprocessed Bitcoin price data.\n",
    "    \"\"\"\n",
    "    file_path = 'data/processed/bitcoin_preprocessed.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Preprocessed data file not found at {file_path}. Run preprocess.py first.\")\n",
    "    \n",
    "    return pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create technical indicators and features for Bitcoin price prediction.\n",
    "    \"\"\"\n",
    "    # Make a copy of the dataframe\n",
    "    data = df.copy()\n",
    "    print(f\"Initial data shape: {data.shape}\")\n",
    "    \n",
    "    # Price-based features\n",
    "    data['returns'] = data['close'].pct_change()\n",
    "    data['log_returns'] = np.log(data['close'] / data['close'].shift(1))\n",
    "    \n",
    "    # Volatility features\n",
    "    data['volatility_1h'] = data['returns'].rolling(window=1).std() * np.sqrt(24)\n",
    "    data['volatility_24h'] = data['returns'].rolling(window=24).std() * np.sqrt(24)\n",
    "    \n",
    "    # Simple Moving Averages\n",
    "    data['sma_6h'] = data['close'].rolling(window=6).mean()\n",
    "    data['sma_12h'] = data['close'].rolling(window=12).mean()\n",
    "    data['sma_24h'] = data['close'].rolling(window=24).mean()\n",
    "    \n",
    "    # Exponential Moving Averages\n",
    "    data['ema_6h'] = data['close'].ewm(span=6, adjust=False).mean()\n",
    "    data['ema_12h'] = data['close'].ewm(span=12, adjust=False).mean()\n",
    "    data['ema_24h'] = data['close'].ewm(span=24, adjust=False).mean()\n",
    "    \n",
    "    # MACD\n",
    "    data['macd'] = data['ema_12h'] - data['ema_24h']\n",
    "    data['macd_signal'] = data['macd'].ewm(span=9, adjust=False).mean()\n",
    "    data['macd_hist'] = data['macd'] - data['macd_signal']\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = data['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    \n",
    "    # Check if loss contains any zeros before division\n",
    "    if (loss == 0).any():\n",
    "        print(\"Warning: Division by zero in RSI calculation\")\n",
    "        # Replace zeros with a small value to avoid division by zero\n",
    "        loss = loss.replace(0, 1e-10)\n",
    "    \n",
    "    rs = gain / loss\n",
    "    data['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    data['bb_middle'] = data['close'].rolling(window=20).mean()\n",
    "    data['bb_std'] = data['close'].rolling(window=20).std()\n",
    "    data['bb_upper'] = data['bb_middle'] + 2 * data['bb_std']\n",
    "    data['bb_lower'] = data['bb_middle'] - 2 * data['bb_std']\n",
    "    data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']\n",
    "    \n",
    "    # Volume features\n",
    "    data['volume_change'] = data['volume'].pct_change()\n",
    "    data['volume_ma_6h'] = data['volume'].rolling(window=6).mean()\n",
    "    data['volume_ma_24h'] = data['volume'].rolling(window=24).mean()\n",
    "    data['volume_ratio'] = data['volume'] / data['volume_ma_24h']\n",
    "    \n",
    "    # Time-based features (hour of day, day of week)\n",
    "    data['hour'] = data.index.hour\n",
    "    data['day_of_week'] = data.index.dayofweek\n",
    "    data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Count NaN values before dropping\n",
    "    nan_counts = data.isna().sum()\n",
    "    print(\"NaN counts per column:\")\n",
    "    print(nan_counts)\n",
    "    print(f\"Total rows with at least one NaN: {data.isna().any(axis=1).sum()}\")\n",
    "    \n",
    "    # Change to more selective NaN removal\n",
    "    # Instead of dropping all rows with any NaN, keep rows with essential data\n",
    "    essential_columns = ['close', 'volume', 'returns', 'rsi_14', 'macd']\n",
    "    data_before_dropna = data.shape[0]\n",
    "    data = data.dropna(subset=essential_columns)\n",
    "    data_after_dropna = data.shape[0]\n",
    "    print(f\"Rows before dropna: {data_before_dropna}, after dropna: {data_after_dropna}\")\n",
    "    \n",
    "    # If still losing too many rows, consider filling NaNs instead\n",
    "    if data_after_dropna < 100:  # Arbitrary threshold\n",
    "        print(\"Too many rows dropped, attempting to fill NaNs instead\")\n",
    "        data = df.copy()\n",
    "        # Apply features again but fill NaNs for rolling calculations\n",
    "        # This is a simplified example\n",
    "        data['returns'] = data['close'].pct_change().fillna(0)\n",
    "        # ... repeat other feature calculations with NaN filling ...\n",
    "    \n",
    "    print(f\"Final data shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating features...\")\n",
    "    # Load preprocessed data\n",
    "    bitcoin_data = load_preprocessed_data()\n",
    "    \n",
    "    # Add basic data inspection\n",
    "    print(f\"Loaded preprocessed data shape: {bitcoin_data.shape}\")\n",
    "    print(f\"Loaded preprocessed data columns: {bitcoin_data.columns.tolist()}\")\n",
    "    print(f\"First few rows of preprocessed data:\")\n",
    "    print(bitcoin_data.head())\n",
    "    \n",
    "    # Check for NaN values in input data\n",
    "    print(f\"NaN values in preprocessed data: {bitcoin_data.isna().sum().sum()}\")\n",
    "    \n",
    "    # Create features\n",
    "    bitcoin_features = create_features(bitcoin_data)\n",
    "    \n",
    "    # Save features\n",
    "    bitcoin_features.to_csv('data/processed/bitcoin_features.csv')\n",
    "    print(f\"Features created and saved to data/processed/bitcoin_features.csv ({len(bitcoin_features)} rows)\")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
